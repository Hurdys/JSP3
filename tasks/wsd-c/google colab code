Here is the link to Google Colab: https://colab.research.google.com/drive/1yyH8mIXcxfvdz-Y_Dbwn9Z54pnOOAnBH?usp=sharing


The code:

!sudo apt-get update && sudo apt-get install -y zstd

!curl -fsSL https://ollama.com/install.sh | sh

!pip install ollama wn

import subprocess
import threading
import time
import ollama

def run_ollama():
    subprocess.Popen(["ollama", "serve"])

thread = threading.Thread(target=run_ollama)
thread.daemon = True
thread.start()
time.sleep(5)

!ollama pull qwen2.5:3b

import wn
wn.download('oewn:2024')

import subprocess
import threading
import time
import ollama
import wn
import argparse
import logging
import re
import json
from ollama import generate
import wn
from tqdm import tqdm
import os
from collections import Counter
import datetime
import statistics

logger = logging.getLogger(__name__)

def parse_arguments():
    parser = argparse.ArgumentParser(description="Tag concepts in the JSON corpus using ollama and wordnet.")
    parser.add_argument("range", help="The range of text to tag, in the format from:to")
    parser.add_argument("json_file", help="Path to the JSON corpus file")
    parser.add_argument("--dry-run", action="store_true", help="Print the selected tags without updating JSON")
    parser.add_argument("-m", "--model", default="qwen2.5:3b", help="Ollama model (default: qwen2.5:3b)")
    parser.add_argument("--wn-only", action="store_true", help="Use only WordNet meanings, exclude additional tags")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output for detailed logging")
    parser.add_argument("--context", type=int, default=2, help="Number of sentences before and after to include in context (default: 2)")
    return parser.parse_args()

def generate_and_extract(prompt, model='qwen2.5:3b'):
    result = generate(model=model, prompt=prompt, options={"temperature": 0.0})
    response = result['response']
    think_match = re.search(r'<thinking>(.*?)</thinking>', response, re.DOTALL)
    if think_match:
        thinking = think_match.group(1).strip()
        cleaned_response = re.sub(r'<thinking>.*?</thinking>', '', response, flags=re.DOTALL).strip()
    else:
        thinking = None
        cleaned_response = response.strip()
    return thinking, cleaned_response

def load_corpus(json_path):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"Loaded: {len(data.get('sent', {}))} sentences, {len(data.get('conc', {}))} concepts")
    return data

def get_sentences_and_concepts(data, min_sid, max_sid):
    sentences = []
    for sid_str in data.get('sent', {}):
        sid = int(sid_str)
        if min_sid <= sid <= max_sid:
            sent = data['sent'][sid_str].copy()
            sent['sid'] = sid
            sent_concepts = {}
            if sid_str in data.get('conc', {}):
                sent_concepts = data['conc'][sid_str]
            sent['concepts'] = sent_concepts
            sentences.append(sent)
    return sentences

def get_sids(from_sid, to_sid, margin):
    return list(range(from_sid - margin, to_sid + margin + 1))

def process_concept(concept, wn_lang='en', args=None):
    lemma = concept['clemma']
    meanings = {}

    try:
        ewn = wn.Wordnet('oewn:2024', lang=wn_lang)
    except Exception as e:
        print(f"Warning: Could not load WordNet with lang='{wn_lang}': {e}")
        ewn = wn.Wordnet('oewn:2024') 

    synsets = ewn.synsets(lemma)

    for ss in synsets:
        defn = ss.definition() or ""
        lemmas = ", ".join(ss.lemmas())
        examples = "; ".join(ss.examples()) if ss.examples() else ""
        example_text = f" ({examples})" if examples else ""
        meanings[ss.id] = f"[{lemmas}] {defn}{example_text}"

    if args is None or not args.wn_only:
        meanings.update({
            'per': 'name of a person not in wordnet (_Irene_ arrived. Captain _Vantoch_ laughed.)',
            'org': 'name of an organization not in wordnet (I work at _IBM_)',
            'dat': 'date/time that is not in wordnet (It starts at _2pm_)',
            'loc': 'name of a place not in wordnet (We study in _Olomouc_)',
            'oth': 'other name not in wordnet (I use a _Thinkpad_)',
            'year': 'name of a year not in wordnet (I was born in _1967_)',
            'num': 'number not in wordnet (There were _42_ of them)',
            'e': 'the word was not tokenized or lemmatized correctly (\'I saw three _does_\' lemmatized as _do_ not _doe_)',
            'w': 'wordnet does not have the correct sense (\'I program in _python_\' meaning \'the computer language\')',
            'x': 'this is a closed class word (preposition, dummy it/there, relative pronoun, passive or progressive be/have, punctuation, love catlove youloveloa...). Or it is part of a multiword expression or it is an inappropriate multiword expression. (\'Kim scored a _hat_ trick\' _hat_ should be part of _hat trick_)'
        })


    return lemma, meanings

def construct_prompt(context, lemma, meanings):
    options = "\n".join([f"{key}: {value}" for key, value in meanings.items()])
    return f"""You are a precise linguistic annotator doing word sense disambiguation.

Context (several sentences around the target word):
> {context}

Target lemma: _{lemma}_

Choose **exactly one** of the following senses that best fits the lemma in this context.
If none of the WordNet senses fit well, prefer the special tags (per, loc, org, oth, w, x, e, bio, etc.).

Rules:
- Use 'per', 'loc', 'org', 'oth' for proper names that are clearly people/places/organizations/other
- Use 'w' ONLY when WordNet has senses, but NONE of them is appropriate in context
- Use 'x' for function words, punctuation, closed-class items, or when the token is part of a multi-word expression
- Use 'e' only for obvious tokenization/lemmatization errors
- Be conservative: do NOT guess rare senses unless context strongly supports them

Options:
{options}

First think step-by-step inside <thinking>...</thinking>, explain your reasoning briefly.
Then on a new line output only the chosen key (example: per or oewn-12345678-n).
Nothing else after that.
"""

def construct_context(index, sentences, context_size):
    start = max(0, index - context_size)
    end = min(len(sentences), index + context_size + 1)
    current_sent = sentences[index].get('text', '')
    #expand if current sentence is short
    if len(current_sent.split()) < 40:
        start = max(0, start - 1)
        end = min(len(sentences), end + 1)
    context_texts = [sent.get('text', '') for sent in sentences[start:end]]
    return ' '.join(context_texts)

def disambiguate(context, lemma, meanings, model):
    prompt = construct_prompt(context, lemma, meanings)
    thinking, cleaned_response = generate_and_extract(prompt, model)
    logger.debug(f"Prompt: {prompt}")
    if thinking:
        logger.debug(f"Model thinking: {thinking}")
    logger.info(f"Model response: {cleaned_response}")
    selected_key = cleaned_response.strip()
    if selected_key in meanings:
        return selected_key, meanings[selected_key]
    return None, None

def sentimentalize(context, lemma, model, gloss=''):
    if gloss:
        gloss = f' ({gloss})'
    sentiment_prompt = f"""You are assigning lexical sentiment ONLY for the word itself in isolation, NOT the overall sentence sentiment.

Target word: _{lemma}_ {gloss}

Context (for disambiguation only, ignore for sentiment value):
> {context}

Assign a number from -100 to +100 representing inherent sentiment of this lexical item:
  0   = neutral (most words, all technical/biological terms, names, numbers)
 +34  = mildly positive
 +64  = clearly positive (good, nice, great…)
 +95  = very positive (fantastic, wonderful…)
-34  = mildly negative
-64  = clearly negative (bad, poor…)
-95  = very negative (awful, terrible…)

Rules:
- Names of people, places, organizations → almost always 0
- Scientific / biological terms (species, anatomy, etc.) → 0
- Do NOT take irony, context modifiers, or story tone into account
- Evaluate the lemma as a dictionary entry, not as it appears in this particular sentence

First think briefly in <thinking>...</thinking>, then output ONLY the number.
"""
    thinking, sentiment_response = generate_and_extract(sentiment_prompt, model)
    logger.debug(f"Sentiment prompt: {sentiment_prompt}")
    if thinking:
        logger.debug(f"Model thinking: {thinking}")
    logger.info(f"Sentiment response: {sentiment_response}")
    try:
        score = float(sentiment_response)
    except ValueError:
        score = None
    return score

def main(range_str, json_file, model, context_window_size, dry_run, verbose, wn_only):
    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)
    logging.getLogger("httpx").setLevel(logging.WARNING)

    class DummyArgs:
        def __init__(self, wn_only_val):
            self.wn_only = wn_only_val

    local_args = DummyArgs(wn_only)
    data = load_corpus(json_file)
    from_sid, to_sid = map(int, range_str.split(':'))
    sids = get_sids(from_sid, to_sid, context_window_size)
    sentences = get_sentences_and_concepts(data, min(sids), max(sids))
    total_concepts = sum(len(s['concepts']) for s in sentences)
    print(f"Processing {total_concepts} concepts with context_size={context_window_size}...")

    tagged = {}
    run_stats = {
        'context_size': context_window_size,
        'start_time': datetime.datetime.now().isoformat(),
        'processed_sentences': 0,
        'total_concepts': total_concepts,
        'tag_counts': Counter(),
        'sentiments': [],
        'non_zero_sentiments': 0,
        'per_sentence': {}
    }

    for i, sentence in tqdm(enumerate(sentences), total=len(sentences), desc="Sentences"):
        if sentence['sid'] < from_sid or sentence['sid'] > to_sid:
            continue

        text_context = construct_context(i, sentences, context_window_size)
        sid_str = str(sentence['sid'])
        tagged[sid_str] = {'concepts': {}}

        context_word_count = len(text_context.split())

        sentence_stats = {
            'sid': sentence['sid'],
            'context_sentences': len(text_context.split()) // 20 + 1,
            'context_words': context_word_count,
            'concept_count': len(sentence['concepts']),
            'tags': [],
            'sentiments': []
        }

        for sub_concept_key, concept_data_dict in sentence['concepts'].items():
            lemma, meanings = process_concept(concept_data_dict, args=local_args)
            selected_key, selected_value = disambiguate(text_context, lemma, meanings, model)
            sentiment = None
            if selected_key and selected_key not in ['x', 'e']:
                sentiment = sentimentalize(text_context, lemma, model, selected_value)

            print(f"\nSID {sid_str}, Sub-Concept Key: {sub_concept_key}:")
            print(f"  Lemma: {lemma}")
            print(f"  Context words: {context_word_count}")
            print(f"  Selected key: {selected_key}")
            if selected_key:
                print(f"  Selected value: {selected_value[:100]}...")
                if sentiment is not None:
                    print(f"  Sentiment: {sentiment}")
            print("-" * 60)

            if not dry_run:
                tagged[sid_str]['concepts'][sub_concept_key] = {
                    'clemma': lemma,
                    'tag': selected_key,
                    'sentiment': sentiment
                }

            if selected_key:
                run_stats['tag_counts'][selected_key] += 1
                sentence_stats['tags'].append(selected_key)

            if sentiment is not None:
                run_stats['sentiments'].append(sentiment)
                sentence_stats['sentiments'].append(sentiment)
                if sentiment != 0:
                    run_stats['non_zero_sentiments'] += 1

        run_stats['processed_sentences'] += 1
        run_stats['per_sentence'][sid_str] = sentence_stats

    run_stats['end_time'] = datetime.datetime.now().isoformat()
    if run_stats['sentiments']:
        sentiments_list = run_stats['sentiments']
        run_stats['sentiment_stats'] = {
            'mean': statistics.mean(sentiments_list),
            'median': statistics.median(sentiments_list),
            'min': min(sentiments_list),
            'max': max(sentiments_list),
            'count_non_zero': run_stats['non_zero_sentiments']
        }

    #summarz
    print("\n" + "="*60)
    print(f"STATISTICS FOR CONTEXT SIZE = {context_window_size}")
    print(f"Processed sentences: {run_stats['processed_sentences']}")
    print(f"Total concepts: {run_stats['total_concepts']}")

    print("\nTag distribution:")
    for tag, count in sorted(run_stats['tag_counts'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {tag:>12}: {count:>4}")

    if 'sentiment_stats' in run_stats:
        ss = run_stats['sentiment_stats']
        print("\nSentiment statistics:")
        print(f"  Mean: {ss['mean']:.2f}")
        print(f"  Median: {ss['median']:.2f}")
        print(f"  Min/Max: {ss['min']}/{ss['max']}")
        print(f"  Non-zero count: {ss['count_non_zero']}")

    print("="*60 + "\n")

    #save tagged data
    if not dry_run:
        base_name = os.path.splitext(json_file)[0]
        output_path = f"{base_name}_tagged_context{context_window_size}.json"
        data_tagged = data.copy()
        for sid_str, updates in tagged.items():
            if sid_str in data_tagged.get('sent', {}):
                if 'concepts' in data_tagged['sent'][sid_str]:
                    data_tagged['sent'][sid_str]['concepts'].update(updates['concepts'])
                else:
                    data_tagged['sent'][sid_str]['concepts'] = updates['concepts']
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data_tagged, f, indent=2)
        print(f"Tagged JSON saved to: {output_path}")

        #stats
        stats_path = f"{base_name}_stats_context{context_window_size}.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(run_stats, f, indent=2, ensure_ascii=False)
        print(f"Statistics saved to: {stats_path}")

#test diff context sizes
context_sizes = [0, 1, 2, 3]
for size in context_sizes:
    print(f"\n=== Testing context size {size} ===")
    main(
        range_str="110001:110006",
        json_file="twwtn-en_human (1).json",
        model="qwen2.5:3b",
        context_window_size=size,
        dry_run=False,
        verbose=True,
        wn_only=False
    )
